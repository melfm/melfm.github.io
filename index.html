<!DOCTYPE html>
<html>
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <meta http-equiv="X-UA-Compatible" content="chrome=1">
  <title>melfm.github.io by melfm</title>
  <link rel="stylesheet" href="css/style.css">
  <meta name="viewport" content="width=device-width">
  <style type="text/css">

  div.paper {
    clear: both;
    margin-top: 0.3em;
    margin-bottom: 0.3em;
    <!--border: 1px solid #ddd;-->
    background: #fff;
    padding: 1em 1em 1em 1em;
    <!--height: 180px-->
  }

  div.paper div {
    padding-left: 280px;
  }
  div.paper div.wide {
    padding-right: 120px;
  }
  img.paper {
    margin-bottom: 0.5em;
    margin-right: 1.0em;
    float: left;
    width: 260px;
    height: 180px;
  }

  div.relative {
      position: relative;
      left: 30px;
  }
  pink {
    color: hotpink;
  }

  </style>
</head>

<body>
  <div class="wrapper">
    <header>
      <h1>Melissa Mozifian</h1>
      <img src="docs/images/DSC_4216.jpg" alt="avod">
      <p> </p>

    </header>
    <div class="relative">
      <div>
        <h2>About </h2>
          <p>
            I am currently a Master student in the
            <a href="http://wavelab.uwaterloo.ca/">Autonomous Vehicles Lab</a> at the Department of Mechanical and Mechantronics Engineering at the
            <a href="https://uwaterloo.ca/">Univeristy of Waterloo  </a>supervised by
            <a href="https://scholar.google.ca/citations?user=CwgGTXMAAAAJ"> Prof. Steven Waslander </a>.
            I'm interested in Computer Vision and Machine Learning. My current projects focus on 3D object detection for autonomous driving using Deep Learning.
          </p>
          <p>
          My background is software engineering, I got my BSc in Computer Science from the <a href=" https://www.st-andrews.ac.uk/">Univeristy of St Andrews </a> in 2014.
          I am currently looking for research opportunities in the areas such as object detection and tracking, semantic scene segmentation, generative models and other topics related to scene understanding for autonomous driving.
          </p>
          <p> Contact : melissafm24 at gmail dot com</p>

          <p class="view"><a href="docs/cv.pdf" title="CV">[CV] </a> <a href="https://github.com/melfm">[GitHub]</a></p>

      </div>
    </div>
    <section>
      <h2> Current Projects </h2>
      <ul>
      <li>
      3D Single Stage Object Detection for Autonomous Driving
      <blockquote>
        AVOD-SSD <a href = "https://github.com/melfm/avod-ssd"> <b><pink>Code</pink></b> </a>
      </blockquote>
      <li>
      Real-time 3D Object Detection for Autonomous Driving
      <blockquote>
        3D object detection (Master thesis) </li>
      </blockquote>
      </ul>

      <h2> Publications </h2>
  <div class="panel-body">
    <!-- Begin of a paper -->
    <div class="paper">
      <table>
        <tr>
          <td>
            <img class="paper"  src="docs/images/detection_sample.png" alt="avod"/>
          </td>
          <td class="paper">
          <a href="https://arxiv.org/abs/1712.02294"><b>Joint 3D Proposal Generation and Object Detection from View Aggregation</b> </a> <i>arXiv preprint</i></br>
            Jason Ku, <b>Melissa Mozifian</b>, Ali Harakeh, Jungwook Lee, Steven Waslander</br>
            <i>International Conference on Intelligent Robots and Systems (<b>IROS</b>), 2018 (Submitted)</i></br>
            <a href = "https://github.com/kujason/avod"> <b><pink>Code</pink></b> </a>/
            <a href = "http://www.cvlibs.net/datasets/kitti/eval_object.php?obj_benchmark=3d" target="_blank"><font>Kitti Benchmark (AVOD) </font></a>/
            <a href = "https://youtu.be/Q1f-s6_yHtw"> Paper Video </a>/
            <a href = "https://youtu.be/M5J65pSkjA8"> More Video</a>

            <!-- <a href = "https://youtu.be/POqBiiLaslk" target="_blank"><font>Youtube</font></a>
             / <a href = "files/mv3d/mv3d_kitti.mp4" target="_blank"><font> Video </font></a></br>//-->
            <pre id="cvpr17chen"  xml:space="preserve" style="display: none;">
            </pre>
          </td>
          </tr>
        </table>
    </div>
    <!-- Begin of a paper -->
    <div class="paper">
      <table>
        <tr>
          <td>
            <img class="paper"  src="docs/images/Pelican.jpg" alt="avod" width="200" height="153" />
          </td>
          <td class="paper">
            <b>Deep Learning a Quadrotor Dynamic Model for Multi-Step Prediction</b></br>
            Nima Mohajerin,<b> Melissa Mozifian</b> and Steven Waslander</br>
            <i>International Conference on Computer Vision on Robotics and Automation (<b>ICRA</b>), 2018</i></br>
            <a href = "https://github.com/wavelab/pelican_dataset" target="_blank"><font>Quadrotor Dataset</font></a>
            <br/>
            </pre>
          </td>
          </tr>
        </table>
    </div>
  </div>
    <h2> Teaching Assistant </h2>
      <ul>
      </blockquote>
      <li><a href="http://wavelab.uwaterloo.ca/?page_id=267">Autonomous Mobile Robots [Mechanical and Mechantronics Engineering Course] </a>
      <blockquote>
        <p> Teaching assistant for the course <i> Autonomous Mobile Robots </i> covering topics such as State Space Modeling, Coordinate Transforms, Motion Modeling, Estimation: Bayes, Kalman, EKF, Particle Filters, Mapping: Localization, Mapping, SLAM and Path Planning</p>
      </blockquote>
      </blockquote>
      <li><a href="https://github.com/jzarnett/ece459">Programming for Performance [Electrical and Computer Engineering Course]</a>
      <blockquote>
        <p> Teaching assistant for the course <i> Programming for performance </i> covering programming with OpenCL.</p>
      </blockquote>
      </ul>
    <h2> Past Projects </h2>
      <ul>
      </blockquote>
      <li><a href="http://www.autonomoose.net/"> Autonomoose</a>
      <blockquote>
      <p>
        Worked on deploying our 3D object detector, AVOD, with <a href="http://www.ros.org/">ROS</a> integration on our self-driving car <a href="https://www.youtube.com/watch?v=gp9Eu50gxO4"> See video </a> </li>
      </p>
      </blockquote>
      <li><a href="https://github.com/melfm/dukecone">Neural Network based EKF Localization</a>
      <blockquote>
        <p>  Object-based localization method with neural networks using <a href="https://pjreddie.com/darknet/yolo/">YOLO</a> (real-time object detector) to localize objects in the camera frame. Using these detected objects as features, an Extended Kalman Filter was used to estimate the robot pose.<a href="https://www.youtube.com/watch?v=OcQGvapoJCg"> See video </a>
      </p>
      </blockquote>
      <li><a href="https://github.com/melfm/kaggle-seizure-prediction">Kaggle Seizure Prediction Challenge</a>
      <blockquote>
      <p> Electrical brain activity (EEG) based seizure forecasting systems using machine learning methods such as Gradient Boosting, Convolutional Neural Networks (CNN) and Recurrent Neural Networks (RNN). See the <a href="https://www.kaggle.com/c/melbourne-university-seizure-prediction"> challenge</a>

      </p>
      </blockquote>
      </blockquote>
      <li><a href="https://github.com/melfm/ece-course-proj/blob/master/report/modelling-quadrotor-dynamics.pdf">Modelling Quadrotor Dynamics Using Neural Networks</a>
      <blockquote>
      <p> Using deep recurrent neural network architecture to model quadrotor dynamics with autoencoders as a pre-training technique.
      </p>
      </blockquote>
      <li><a href="https://info.cs.st-andrews.ac.uk/student-handbook/files/project-library/sh/Mozifian.pdf">Affective Mirror (BSc Dissertation)</a>
      <blockquote>
      <p>  The goal of this project was to develop a
            system with the ability to detect emotions such as happiness,
            sadness, surprise and excitement by analysing the facial cues using SVMs.<a href="https://www.youtube.com/watch?v=cIABgfEnbGs"> See video </a>
      </p>
      </blockquote>
      <li><a href="https://github.com/melfm/pulse-detector">Webcam-based Pulse Detector</a>
      <blockquote>
      <p> This application uses <a href="https://pjreddie.com/darknet/yolo/">OpenCV</a>  to detect the user's face to isolate the forehead region. Data is collected from this region over time to estimate the user's heart rate by measuring average optical intensity in the forehead location. Physiological data can be estimated this way due to the optical absorption characteristics of (oxy-) haemoglobin <a href="https://www.osapublishing.org/oe/abstract.cfm?uri=oe-16-26-21434"> See Plethysmographic imaging </a>
      </p>
      </blockquote>
      </blockquote>
      <li><a href="https://github.com/melfm/Idris-dev">Idris, A Dependently Typed Functional Programming Language</a>
      <blockquote>
      <p> I contributed to <a href="https://www.idris-lang.org/">Idris,</a> a general-purpose functional programming language with dependent types. I implemented a  Secure SQL Query system Using Dependent Types.
      </p>
      </blockquote>
      </li>
      </ul>
  </section>
</div>
</body></html>
